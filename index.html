<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conversational AI with Intent Detection</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }
        
        .container {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            max-width: 1200px;
            width: 100%;
        }
        
        h1 {
            text-align: center;
            color: #667eea;
            margin-bottom: 10px;
            font-size: 2em;
        }
        
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 0.9em;
        }

        .mode-toggle {
            text-align: center;
            margin-bottom: 20px;
        }

        .mode-toggle label {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            font-weight: 600;
            color: #667eea;
            cursor: pointer;
        }

        .mode-toggle input[type="checkbox"] {
            width: 50px;
            height: 25px;
            appearance: none;
            background: #ccc;
            border-radius: 25px;
            position: relative;
            cursor: pointer;
            transition: 0.3s;
        }

        .mode-toggle input[type="checkbox"]:checked {
            background: #10b981;
        }

        .mode-toggle input[type="checkbox"]::before {
            content: '';
            position: absolute;
            width: 21px;
            height: 21px;
            border-radius: 50%;
            background: white;
            top: 2px;
            left: 2px;
            transition: 0.3s;
        }

        .mode-toggle input[type="checkbox"]:checked::before {
            left: 27px;
        }
        
        .video-section {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        
        .video-wrapper {
            flex: 1;
            min-width: 280px;
            position: relative;
        }
        
        video {
            width: 100%;
            border-radius: 15px;
            background: #000;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        
        .video-label {
            position: absolute;
            top: 10px;
            left: 10px;
            background: rgba(0,0,0,0.7);
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: bold;
        }

        .avatar-status {
            position: absolute;
            bottom: 10px;
            left: 10px;
            right: 10px;
            background: rgba(0,0,0,0.8);
            color: white;
            padding: 10px;
            border-radius: 10px;
            font-size: 0.85em;
            display: none;
        }

        .avatar-status.speaking {
            display: block;
            background: rgba(16, 185, 129, 0.9);
        }

        .avatar-status.listening {
            display: block;
            background: rgba(59, 130, 246, 0.9);
        }
        
        .controls {
            text-align: center;
            margin-bottom: 30px;
        }
        
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 40px;
            border-radius: 30px;
            font-size: 1.1em;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: bold;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
            margin: 0 10px;
        }
        
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 7px 20px rgba(102, 126, 234, 0.6);
        }
        
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
        }
        
        .status-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }
        
        .status-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        
        .status-card h3 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .status-value {
            font-size: 2em;
            font-weight: bold;
            margin: 10px 0;
        }
        
        .status-value.true {
            color: #10b981;
        }
        
        .status-value.false {
            color: #ef4444;
        }
        
        .detail {
            font-size: 0.9em;
            color: #666;
            margin-top: 10px;
        }
        
        .visualization {
            background: #f9fafb;
            padding: 20px;
            border-radius: 15px;
            margin-bottom: 20px;
        }
        
        .visualization h3 {
            color: #667eea;
            margin-bottom: 15px;
        }
        
        canvas {
            width: 100%;
            height: 100px;
            background: white;
            border-radius: 10px;
        }

        .conversation-box {
            background: #f9fafb;
            padding: 20px;
            border-radius: 15px;
            margin-bottom: 20px;
            max-height: 300px;
            overflow-y: auto;
        }

        .conversation-box h3 {
            color: #667eea;
            margin-bottom: 15px;
        }

        .message {
            margin-bottom: 15px;
            padding: 12px 15px;
            border-radius: 15px;
            max-width: 80%;
            animation: slideIn 0.3s ease;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .message.user {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
            text-align: right;
        }

        .message.avatar {
            background: #e5e7eb;
            color: #1f2937;
        }

        .message-label {
            font-size: 0.75em;
            opacity: 0.8;
            margin-bottom: 5px;
        }
        
        .logs {
            background: #1e293b;
            color: #10b981;
            padding: 20px;
            border-radius: 15px;
            max-height: 200px;
            overflow-y: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
        }
        
        .log-entry {
            margin-bottom: 5px;
            padding: 5px;
            border-left: 3px solid #10b981;
            padding-left: 10px;
        }
        
        .log-entry.warning {
            border-left-color: #f59e0b;
            color: #f59e0b;
        }

        .log-entry.avatar {
            border-left-color: #3b82f6;
            color: #3b82f6;
        }
        
        .indicator {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse 2s infinite;
        }
        
        .indicator.active {
            background: #10b981;
        }
        
        .indicator.inactive {
            background: #ef4444;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        
        .metrics {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }
        
        .metric-badge {
            background: white;
            padding: 8px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸ¤– Conversational AI with Intent Detection</h1>
        <p class="subtitle">Natural two-way conversation with smart interruption handling</p>

        <div class="mode-toggle">
            <label>
                <span>Detection Only</span>
                <input type="checkbox" id="conversationMode" checked>
                <span>Full Conversation</span>
            </label>
        </div>
        
        <div class="video-section">
            <div class="video-wrapper">
                <video id="videoElement" autoplay playsinline></video>
                <div class="video-label">ðŸ“¹ You</div>
                <div class="avatar-status" id="avatarStatus">ðŸ¤– Avatar is listening...</div>
            </div>
        </div>
        
        <div class="controls">
            <button id="startBtn">Start Conversation</button>
            <button id="stopBtn" disabled>Stop</button>
        </div>

        <div class="conversation-box" id="conversationBox" style="display: none;">
            <h3>ðŸ’¬ Conversation History</h3>
            <div id="messages"></div>
        </div>
        
        <div class="status-grid">
            <div class="status-card">
                <h3>ðŸŽ¤ User Done Speaking</h3>
                <div class="status-value false" id="userDone">FALSE</div>
                <div class="detail">
                    <span class="indicator inactive" id="doneIndicator"></span>
                    <span id="doneReason">Waiting for input...</span>
                </div>
            </div>
            
            <div class="status-card">
                <h3>ðŸ’¬ User About to Speak</h3>
                <div class="status-value false" id="aboutToSpeak">FALSE</div>
                <div class="detail">
                    <span class="indicator inactive" id="speakIndicator"></span>
                    <span id="speakReason">No activity detected</span>
                </div>
            </div>
            
            <div class="status-card">
                <h3>ðŸ“Š Speech Status</h3>
                <div class="metrics">
                    <div class="metric-badge">Speaking: <strong id="isSpeaking">No</strong></div>
                    <div class="metric-badge">Pause: <strong id="pauseDuration">0s</strong></div>
                    <div class="metric-badge">Lips: <strong id="lipsMoving">Inactive</strong></div>
                </div>
            </div>
        </div>
        
        <div class="visualization">
            <h3>ðŸ”Š Audio Level Visualization</h3>
            <canvas id="audioCanvas"></canvas>
        </div>
        
        <div class="logs" id="logContainer">
            <div class="log-entry">> System initialized. Ready to start conversation...</div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.4.1633559619/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.3.1620248357/camera_utils.js"></script>
    
    <script>
        // Configuration
        const CONFIG = {
            DONE_THRESHOLD: 3.0,
            PAUSE_THRESHOLD: 1.5,
            ABOUT_TO_SPEAK_THRESHOLD: 0.3,
            AUDIO_THRESHOLD: -50,
            LIP_MOVEMENT_THRESHOLD: 0.015
        };

        // Global state
        let mediaStream = null;
        let audioContext = null;
        let analyser = null;
        let faceMesh = null;
        let camera = null;
        let isDetecting = false;
        let conversationMode = true;
        
        let isSpeaking = false;
        let lastSpeechTime = 0;
        let silenceStartTime = 0;
        let previousLipDistance = 0;
        let lipMovementDetected = false;
        let avatarIsSpeaking = false;
        let userTranscript = '';
        let speechRecognition = null;

        // Avatar responses
        const avatarResponses = [
            "I understand. Could you tell me more about that?",
            "That's interesting! What made you think of that?",
            "I see. Is there anything specific you'd like to know?",
            "Got it! How can I help you with that?",
            "Thanks for sharing. What would you like to do next?",
            "Interesting point. Would you like me to elaborate?",
            "I hear you. Let me think about that for a moment.",
            "That makes sense. What else is on your mind?"
        ];

        // DOM Elements
        const videoElement = document.getElementById('videoElement');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const audioCanvas = document.getElementById('audioCanvas');
        const audioCtx = audioCanvas.getContext('2d');
        const conversationModeToggle = document.getElementById('conversationMode');
        const conversationBox = document.getElementById('conversationBox');
        const messagesContainer = document.getElementById('messages');
        const avatarStatus = document.getElementById('avatarStatus');

        // Initialize Speech Recognition
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
                speechRecognition = new SpeechRecognition();
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                speechRecognition.lang = 'en-US';

                speechRecognition.onresult = (event) => {
                    let interim = '';
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        if (event.results[i].isFinal) {
                            userTranscript = event.results[i][0].transcript;
                        } else {
                            interim = event.results[i][0].transcript;
                        }
                    }
                };

                speechRecognition.start();
            }
        }

        // Text to Speech
        function speak(text) {
            if (!conversationMode) return;
            
            avatarIsSpeaking = true;
            avatarStatus.className = 'avatar-status speaking';
            avatarStatus.textContent = 'ðŸ¤– Avatar speaking...';
            addMessage(text, 'avatar');
            addLog('ðŸ¤– Avatar responding', false, 'avatar');

            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.0;
            utterance.pitch = 1.0;
            utterance.volume = 1.0;

            utterance.onend = () => {
                avatarIsSpeaking = false;
                avatarStatus.className = 'avatar-status listening';
                avatarStatus.textContent = 'ðŸ¤– Avatar listening...';
                addLog('âœ“ Avatar finished speaking', false, 'avatar');
            };

            window.speechSynthesis.speak(utterance);
        }

        // Add message to conversation
        function addMessage(text, sender) {
            if (!conversationMode) return;

            const message = document.createElement('div');
            message.className = `message ${sender}`;
            
            const label = document.createElement('div');
            label.className = 'message-label';
            label.textContent = sender === 'user' ? 'You' : 'Avatar';
            
            const content = document.createElement('div');
            content.textContent = text;
            
            message.appendChild(label);
            message.appendChild(content);
            messagesContainer.appendChild(message);
            
            conversationBox.scrollTop = conversationBox.scrollHeight;
        }

        // Toggle conversation mode
        conversationModeToggle.addEventListener('change', (e) => {
            conversationMode = e.target.checked;
            conversationBox.style.display = conversationMode ? 'block' : 'none';
            addLog(`Mode: ${conversationMode ? 'Full Conversation' : 'Detection Only'}`);
        });

        // Logging
        function addLog(message, isWarning = false, type = 'normal') {
            const logContainer = document.getElementById('logContainer');
            const entry = document.createElement('div');
            entry.className = `log-entry ${isWarning ? 'warning' : ''} ${type}`;
            const timestamp = new Date().toLocaleTimeString();
            entry.textContent = `[${timestamp}] ${message}`;
            logContainer.insertBefore(entry, logContainer.firstChild);
            if (logContainer.children.length > 20) {
                logContainer.removeChild(logContainer.lastChild);
            }
        }

        // Initialize MediaPipe Face Mesh
        function initFaceMesh() {
            faceMesh = new FaceMesh({
                locateFile: (file) => {
                    return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh@0.4.1633559619/${file}`;
                }
            });
            
            faceMesh.setOptions({
                maxNumFaces: 1,
                refineLandmarks: true,
                minDetectionConfidence: 0.5,
                minTrackingConfidence: 0.5
            });
            
            faceMesh.onResults(onFaceMeshResults);
        }

        // Calculate lip distance
        function calculateLipDistance(landmarks) {
            const upperLip = landmarks[13];
            const lowerLip = landmarks[14];
            
            const distance = Math.sqrt(
                Math.pow(upperLip.x - lowerLip.x, 2) +
                Math.pow(upperLip.y - lowerLip.y, 2)
            );
            
            return distance;
        }

        // Face mesh results callback
        function onFaceMeshResults(results) {
            if (!isDetecting) return;
            
            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
                const landmarks = results.multiFaceLandmarks[0];
                const currentLipDistance = calculateLipDistance(landmarks);
                
                if (previousLipDistance > 0) {
                    const lipChange = Math.abs(currentLipDistance - previousLipDistance);
                    lipMovementDetected = lipChange > CONFIG.LIP_MOVEMENT_THRESHOLD;
                    
                    document.getElementById('lipsMoving').textContent = 
                        lipMovementDetected ? 'Moving' : 'Still';
                }
                
                previousLipDistance = currentLipDistance;
            }
        }

        // Audio analysis
        function analyzeAudio() {
            if (!isDetecting) return;
            
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteFrequencyData(dataArray);
            
            const average = dataArray.reduce((a, b) => a + b) / bufferLength;
            const dB = 20 * Math.log10(average / 255);
            
            visualizeAudio(dataArray);
            
            const currentTime = Date.now() / 1000;
            const wasSpeaking = isSpeaking;
            isSpeaking = dB > CONFIG.AUDIO_THRESHOLD;
            
            document.getElementById('isSpeaking').textContent = isSpeaking ? 'Yes' : 'No';
            
            if (isSpeaking) {
                lastSpeechTime = currentTime;
                silenceStartTime = 0;
                
                // If avatar is speaking and user tries to interrupt
                if (avatarIsSpeaking && conversationMode) {
                    window.speechSynthesis.cancel();
                    avatarIsSpeaking = false;
                    addLog('âš  User interrupted avatar', true);
                }
            } else if (wasSpeaking && !isSpeaking) {
                silenceStartTime = currentTime;
            }
            
            updateIntentDetection(currentTime);
            
            requestAnimationFrame(analyzeAudio);
        }

        // Visualize audio levels
        function visualizeAudio(dataArray) {
            const width = audioCanvas.width = audioCanvas.offsetWidth;
            const height = audioCanvas.height = audioCanvas.offsetHeight;
            
            audioCtx.fillStyle = 'white';
            audioCtx.fillRect(0, 0, width, height);
            
            const barWidth = (width / dataArray.length) * 2.5;
            let barHeight;
            let x = 0;
            
            for (let i = 0; i < dataArray.length; i++) {
                barHeight = (dataArray[i] / 255) * height;
                
                const gradient = audioCtx.createLinearGradient(0, height - barHeight, 0, height);
                gradient.addColorStop(0, '#667eea');
                gradient.addColorStop(1, '#764ba2');
                
                audioCtx.fillStyle = gradient;
                audioCtx.fillRect(x, height - barHeight, barWidth, barHeight);
                
                x += barWidth + 1;
            }
        }

        // Update intent detection
        function updateIntentDetection(currentTime) {
            let userDone = false;
            let userAboutToSpeak = false;
            let doneReason = '';
            let speakReason = '';
            
            let pauseDuration = 0;
            if (silenceStartTime > 0) {
                pauseDuration = currentTime - silenceStartTime;
            }
            
            document.getElementById('pauseDuration').textContent = pauseDuration.toFixed(1) + 's';
            
            // User Done Speaking
            if (!isSpeaking && pauseDuration >= CONFIG.DONE_THRESHOLD && !lipMovementDetected) {
                userDone = true;
                doneReason = `Silence > ${CONFIG.DONE_THRESHOLD}s, no lip movement`;
                
                // Trigger avatar response in conversation mode
                if (conversationMode && !avatarIsSpeaking && userTranscript) {
                    addMessage(userTranscript, 'user');
                    addLog('âœ“ User done speaking detected');
                    
                    setTimeout(() => {
                        const response = avatarResponses[Math.floor(Math.random() * avatarResponses.length)];
                        speak(response);
                    }, 500);
                    
                    userTranscript = '';
                }
            } else if (isSpeaking) {
                doneReason = 'Currently speaking';
            } else if (pauseDuration < CONFIG.DONE_THRESHOLD) {
                doneReason = `Short pause (${pauseDuration.toFixed(1)}s)`;
            } else if (lipMovementDetected) {
                doneReason = 'Lips moving - user thinking';
            }
            
            // User About to Speak
            if (!isSpeaking && lipMovementDetected) {
                userAboutToSpeak = true;
                speakReason = 'Lip movement detected during silence';
                if (!avatarIsSpeaking) {
                    addLog('âš  User about to speak', true);
                }
            } else if (isSpeaking) {
                speakReason = 'Currently speaking';
            } else if (!isSpeaking && pauseDuration < CONFIG.ABOUT_TO_SPEAK_THRESHOLD) {
                userAboutToSpeak = true;
                speakReason = 'Very short pause - likely continuing';
            } else {
                speakReason = 'No activity';
            }
            
            updateStatus('userDone', 'doneIndicator', userDone, doneReason);
            updateStatus('aboutToSpeak', 'speakIndicator', userAboutToSpeak, speakReason);
        }

        // Update status display
        function updateStatus(valueId, indicatorId, value, reason) {
            const valueEl = document.getElementById(valueId);
            const indicatorEl = document.getElementById(indicatorId);
            const reasonEl = document.getElementById(valueId === 'userDone' ? 'doneReason' : 'speakReason');
            
            valueEl.textContent = value ? 'TRUE' : 'FALSE';
            valueEl.className = `status-value ${value ? 'true' : 'false'}`;
            indicatorEl.className = `indicator ${value ? 'active' : 'inactive'}`;
            reasonEl.textContent = reason;
        }

        // Start detection
        async function startDetection() {
            try {
                addLog('Initializing camera and microphone...');
                
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 1280, height: 720 },
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                videoElement.srcObject = mediaStream;
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                analyser.smoothingTimeConstant = 0.8;
                
                const source = audioContext.createMediaStreamSource(mediaStream);
                source.connect(analyser);
                
                initFaceMesh();
                
                camera = new Camera(videoElement, {
                    onFrame: async () => {
                        if (faceMesh) {
                            await faceMesh.send({ image: videoElement });
                        }
                    },
                    width: 1280,
                    height: 720
                });
                camera.start();
                
                if (conversationMode) {
                    initSpeechRecognition();
                    conversationBox.style.display = 'block';
                    avatarStatus.className = 'avatar-status listening';
                    addLog('ðŸ¤– Conversation mode active', false, 'avatar');
                    
                    setTimeout(() => {
                        speak("Hello! I'm your AI assistant. How can I help you today?");
                    }, 1000);
                }
                
                isDetecting = true;
                startBtn.disabled = true;
                stopBtn.disabled = false;
                
                addLog('âœ“ System started successfully');
                analyzeAudio();
                
            } catch (error) {
                addLog(`Error: ${error.message}`, true);
                console.error('Error starting detection:', error);
            }
        }

        // Stop detection
        function stopDetection() {
            isDetecting = false;
            
            if (camera) {
                camera.stop();
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
            }
            
            if (audioContext) {
                audioContext.close();
            }

            if (speechRecognition) {
                speechRecognition.stop();
            }

            if (window.speechSynthesis) {
                window.speechSynthesis.cancel();
            }
            
            videoElement.srcObject = null;
            
            startBtn.disabled = false;
            stopBtn.disabled = true;
            
            addLog('System stopped');
        }

        // Event listeners
        startBtn.addEventListener('click', startDetection);
        stopBtn.addEventListener('click', stopDetection);
    </script>
</body>
</html>